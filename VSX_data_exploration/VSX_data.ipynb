{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c565d4-3442-48eb-8180-0fc964278587",
   "metadata": {},
   "source": [
    "# VSX Catalog Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d98d994-63dc-44ac-9a57-10ea7cb5d577",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/mmpkb8/DataScience/VSX_data_exploration/VSX_data.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mmpkb8/DataScience/VSX_data_exploration/VSX_data.ipynb#ch0000001?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mmpkb8/DataScience/VSX_data_exploration/VSX_data.ipynb#ch0000001?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mmpkb8/DataScience/VSX_data_exploration/VSX_data.ipynb#ch0000001?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aae2df-3e1a-47bc-9786-5f07c179f19f",
   "metadata": {},
   "source": [
    "We need to specify the values used to denote missing or null values, since there is a variable star type of \"NA\" for a type of nova that will throw off the null values in the vartype column otherwise. We also need to specify the dtype for each column to both speed up the data load and account for the mixed value types that appeared in some of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68025a-eca8-4660-a61c-362bd25463a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/vsx_data.csv\", encoding = 'unicode_escape', keep_default_na = False, \n",
    "                 na_values=['NULL','null', 'nan','NaN', \"\"],  dtype={'auid':str, \n",
    "                                                                     'name':str, \n",
    "                                                                     'const':str, \n",
    "                                                                     'ra':float, \n",
    "                                                                     'dec':float, \n",
    "                                                                     'varType':str,\n",
    "                                                                     'maxMag':float, \n",
    "                                                                     'maxPass':str, \n",
    "                                                                     'minMag':float, \n",
    "                                                                     'minPass':str, \n",
    "                                                                     'epoch':float, \n",
    "                                                                     'novaYr':'UInt64', \n",
    "                                                                     'period':float, \n",
    "                                                                     'riseDur':float, \n",
    "                                                                     'specType':str,\n",
    "                                                                     'disc':str})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb74b0-9c9d-4106-8829-9fec78ac0c86",
   "metadata": {},
   "source": [
    "Next, I'm going to drop a few of the columns that don't contain information I plan on using in my analysis. I could have selected only the columns that I needed while reading the csv file, but since the dataset isn't that large and I'm using the majority of them I didn't do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c632e-254e-42b8-9343-edfcaae13ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['epoch', 'novaYr', 'disc'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a8203-ccd5-4678-a9d3-66110ae6b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.name = 'index'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25000e52-ead4-4bcf-8ec1-5a43caf9375a",
   "metadata": {},
   "source": [
    "## Things to look at:\n",
    "\n",
    "1. Data cleanup, what's missing and why\n",
    "3. How many stars have AUIDs assigned?\n",
    "2. Breakdown by constellation\n",
    "- How many stars are missing a variability type?\n",
    "4. How many stars have come from which surveys or other sources?\n",
    "5. Frequency of different types\n",
    "6. Magnitude distribution\n",
    "7. Period distribution\n",
    "8. How many have spectral type, and is it available for the others?\n",
    "9. Any potential correlations or areas for followup?\n",
    "10. Create all-sky map of coordinate distribution?\n",
    "11. Any chance for duplicate stars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216dd32-e03f-45fc-a21d-33376c31c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252211c4-ab30-4c14-a3d7-ac4c2962b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea924860-ff2a-4781-9f4c-b08ff0c386a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_auid = df['auid'].count() / df.shape[0] \n",
    "print(tot_auid * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b329af-e127-42c5-bbb5-c751231c7aa2",
   "metadata": {},
   "source": [
    "Only 3.8 percent of the records have an AUID! AUIDs can be requested for any star, however this implies that there is a significant part of this data set that don't have any obsevations by AAVSO observers in the photometry database. This seems unusually low, so I need to figure out if this is actually correct. Also, most stars don't have a nova year associated, but that is more reasonable since only a fraction of all variable stars could be expected to be variable due to nova activity. This is why I dropped that column.\n",
    "\n",
    "One thing I see that is potentially interesting is that there are at least 1000 records that don't have a variable type set. Not sure if it's because it's not known or just not part of the record when it was uploaded to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef70ee-85c5-4791-a0b2-b61b9afadba1",
   "metadata": {},
   "source": [
    "## Stars without a variability type assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e7308c-aad0-42fb-a678-ad65ab4326c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_type = df[df['varType'].isnull()]\n",
    "print(no_type.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec1cb7-64e3-465e-a131-aaed2a537184",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_type['maxMag'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4658b0-a7d6-4042-bb51-8e5ed174752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=no_type, x=\"maxMag\", kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0954197-8941-4d76-8830-534c018e548d",
   "metadata": {},
   "source": [
    "A lot of the stars without a variable type in VSX are too faint even at their maximum to be observed by a typical amateur's telescope, but there are definitely some in there well within reach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f2d89-42f7-40ff-a960-e39d400ab4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_unknown = no_type.loc[no_type['maxMag'] < 10]\n",
    "print(bright_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eec344-afe5-4b2b-9d8d-02d1fdfc8059",
   "metadata": {},
   "source": [
    "## Sort data - Survey or Other Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cade5b-88c6-45ad-a343-d9e5d9fefdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = \"\"\n",
    "\n",
    "df['source'] = np.where(df['name'].str.contains('NSV'),\n",
    "                                'NSV',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('ASAS'),\n",
    "                                'ASAS',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('OGLE'),\n",
    "                                'OGLE',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('CSS'),\n",
    "                                'CSS',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('2MASS'),\n",
    "                                '2MASS',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('USNO'),\n",
    "                                'USNO',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('HAT'),\n",
    "                                'HAT',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('PTF'),\n",
    "                                'PTF',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('WASP'),\n",
    "                                'WASP',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('GCVS'),\n",
    "                                'GCVS',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('GDS'),\n",
    "                                'GDS',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('KIC'),\n",
    "                                'KIC',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('PNV'),\n",
    "                                'PNV',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('MASTER'),\n",
    "                                'MASTER',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('SWIFT'),\n",
    "                                'SWIFT',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('ZTF'),\n",
    "                                'ZTF',df['source'])\n",
    "df['source'] = np.where(df['name'].str.contains('Gaia'),\n",
    "                                'Gaia',df['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f4b685-21f6-4b47-aca7-72d14e4a85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffc6f3-3380-4b69-b321-43957e463aec",
   "metadata": {},
   "source": [
    "## Breakdown by Constellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29e72d-8f1d-4c80-b1cd-4b9b683f0830",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "df['const'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed8a04-595b-48fa-a347-7df5864a49a8",
   "metadata": {},
   "source": [
    "A lot of the top constellations (Sagittarius, Scutum, Aquila, Ophiuchus, and more) are along the direction of the Milky Way as seen from Earth, which would explain a much higher concentration of stars and star-forming regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea83e13-9b51-4330-8cdb-c963f7efa118",
   "metadata": {},
   "source": [
    "The variability type has a ton of different options, since it can be any of the types defined by VSX, the surveys the data originally came from, or a combination of types if the type isn't certain. For easier analysis, I'm going to refine the types down to basic variable type categories: eclipsing, rotating, microlensing, pulsating, eruptive, cataclysmic, and x-ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78204a8-2d2f-4604-a554-e6b5b62a929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = \"\"\n",
    "\n",
    "eclipsing = ['EA', 'EB', 'EW', 'EP', 'EC', 'ED', 'ESD']\n",
    "rotating = ['ACV', 'BY', 'CTTS/ROT', 'ELL', 'FKCOM', 'HB', 'LERI', 'PSR', 'ROT', 'RS', 'SXARI', 'SXARI/E', 'TTS/ROT', 'WTTS/ROT', 'NSIN ELL']\n",
    "pulsating = ['ACEP', 'ACYG', 'AHB1', 'BCEP', 'BLAP', 'BXCIR', 'CEP', 'CW', 'DCEP', 'DSCT', 'DWLYN', 'GDOR', 'HADS', 'LB', 'LC', 'PPN', 'PVTEL', 'PVTELI', 'roAm', 'roAp', 'RR', 'RV', 'SPB', 'SR', 'SXPHE', 'V361HYA', 'V1093HER', 'ZZ', 'LPV', 'PULS']\n",
    "eruptive = ['BE', 'cPNB', 'CTTS', 'DPV', 'DYPer', 'EXOR', 'FF', 'FSCMa', 'FUOR', 'GCAS', 'IA', 'IB', 'IN', 'INT', 'ISA', 'ISB', 'RCB', 'SDOR', 'TTS', 'UV', 'UVN', 'UXOR', 'WR', 'WTTS', 'ZZA', 'YSO', 'DIP', 'YY']\n",
    "cataclysmic = ['AM', 'CBSS', 'DQ', 'IBWD', 'NA', 'NB', 'NC', 'NL', 'NR', 'SN', 'UG', 'V838MON', 'WDP', 'ZAND', 'CV', 'IBWD', 'VY']\n",
    "xray = ['HMXB', 'IMXB', 'LMXB', 'BHXB', 'XB', 'XJ', 'XN', 'XP']\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "     \n",
    "    if any(substring in str(row['varType']) for substring in eclipsing):\n",
    "        df.loc[index, 'category'] = 'eclipsing'\n",
    "    elif any(substring in str(row['varType']) for substring in rotating):\n",
    "        df.loc[index, 'category'] = 'rotating'\n",
    "    elif any(substring in str(row['varType']) for substring in pulsating):\n",
    "        df.loc[index, 'category'] = 'pulsating'\n",
    "    elif any(substring in str(row['varType']) for substring in eruptive):\n",
    "        df.loc[index, 'category'] = 'eruptive'\n",
    "    elif any(substring in str(row['varType']) for substring in cataclysmic):\n",
    "        df.loc[index, 'category'] = 'cataclysmic'\n",
    "    elif any(substring in str(row['varType']) for substring in xray):\n",
    "        df.loc[index, 'category'] = 'xray'\n",
    "    elif str(row['varType']) == 'VAR':\n",
    "        df.loc[index, 'category'] = 'unspec'\n",
    "    elif str(row['varType']) == 'E':\n",
    "        df.loc[index, 'category'] = 'eruptive'\n",
    "    elif str(row['varType']) == 'M':\n",
    "        df.loc[index, 'category'] = 'cataclysmic'\n",
    "    elif str(row['varType']) == 'L':\n",
    "        df.loc[index, 'category'] = 'pulsating'\n",
    "    elif str(row['varType']) == 'N':\n",
    "        df.loc[index, 'category'] = 'cataclysmic'\n",
    "    else:\n",
    "        df.loc[index, 'category'] = 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa383862-787d-4b4e-8a3a-545bd06f4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace1682-e145-42f3-abc9-5322ef3f2c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(df, x='const', hue='category', \n",
    "             multiple='stack', palette='cubehelix')\n",
    "ax.set_ylabel('count')\n",
    "# Fix the legend so it's not on top of the bars.\n",
    "legend = ax.get_legend()\n",
    "legend.set_bbox_to_anchor((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c58b4-af80-4b6c-a94a-b4740e567ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#one_hot = pd.get_dummies(df['category'])\n",
    "#df = df.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e6d89-9587-4c21-b57a-4e79f5bb81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c066c9d-5f64-4338-9554-6ba1050f6014",
   "metadata": {},
   "source": [
    "Given the min/max mag and period, is there any way to predict star type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3da8a-0ebf-4699-8dad-04b47dfaf3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[['maxMag', 'minMag', 'period', 'category']]\n",
    "df_filtered['category'] = pd.factorize(df_filtered['category'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9e0eb-bfd1-4bd1-8a07-ed6defa8bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered = df[[\"maxMag\", \"minMag\", \"period\", ]\n",
    "y =  np.asarray(df_filtered.pop('category')).astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_filtered, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d7295-c3f7-48f3-8af8-77ce393db898",
   "metadata": {},
   "source": [
    "Let's create a basic neural net to see if any prediction can be made on star category from the minimum and maximum magnitudes, as well as the period given. One thing to take into consideration is that not all magnitudes are in the same band, some were given in visual, V, or G bands, and some were R (red) or B (blue). \n",
    "\n",
    "I'm not sure yet if normalizing any of the data will be useful. We should probably normalize the period since that can vary widely from a fraction of a day to over a year, but I want to try the model first to see what the effects of normalization would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a26cc-29d8-467b-acfc-730047f28ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([ \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(8, activation=tf.nn.softmax)\n",
    "    ]) \n",
    "\n",
    "    # Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(0.001), \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy']) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0ac00-2c8a-4df9-a129-df95f50f11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Fit the model for 10 epochs adding the callbacks\n",
    "    # and save the training history\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02fff92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ee903ba7ed44934e77c9040c093e448ea5668725b8449a9b5ec1b353c4fe35e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
